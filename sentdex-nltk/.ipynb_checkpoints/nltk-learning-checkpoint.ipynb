{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mayankaggarwal/anaconda/lib/python2.7/site-packages/nltk/__init__.pyc\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "#print where nltk directory\n",
    "print(nltk.__file__)\n",
    "# use nltk.download() for downloading all files under nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str=\"Tropical Storm Bill hit the Gulf Coast of the United States in the summer of 2003. The second storm of that Atlantic hurricane season, Bill developed from a tropical wave on June 29 to the north of the Peninsula. It slowly organized as it moved northward, and reached a peak of 60 mph (95 km/h) shortly before making landfall in south-central Louisiana. It produced a moderate storm surge, causing tidal flooding. In Montegut in the southeastern portion of the state, a levee was breached, flooding many homes, and in Florida, two swimmers drowned.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tropical', 'Storm', 'Bill', 'hit', 'the', 'Gulf', 'Coast', 'of', 'the', 'United', 'States', 'in', 'the', 'summer', 'of', '2003', '.', 'The', 'second', 'storm', 'of', 'that', 'Atlantic', 'hurricane', 'season', ',', 'Bill', 'developed', 'from', 'a', 'tropical', 'wave', 'on', 'June', '29', 'to', 'the', 'north', 'of', 'the', 'Peninsula', '.', 'It', 'slowly', 'organized', 'as', 'it', 'moved', 'northward', ',', 'and', 'reached', 'a', 'peak', 'of', '60', 'mph', '(', '95', 'km/h', ')', 'shortly', 'before', 'making', 'landfall', 'in', 'south-central', 'Louisiana', '.', 'It', 'produced', 'a', 'moderate', 'storm', 'surge', ',', 'causing', 'tidal', 'flooding', '.', 'In', 'Montegut', 'in', 'the', 'southeastern', 'portion', 'of', 'the', 'state', ',', 'a', 'levee', 'was', 'breached', ',', 'flooding', 'many', 'homes', ',', 'and', 'in', 'Florida', ',', 'two', 'swimmers', 'drowned', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u'do', u'them', u'his', u'very', u'they', u'not', u'during', u'now', u'him', u'nor', u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u'where', u'few', u'because', u'doing', u'some', u'hasn', u'are', u'our', u'ourselves', u'out', u'what', u'for', u'while', u're', u'does', u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u'were', u'here', u'shouldn', u'hers', u'by', u'on', u'about', u'couldn', u'of', u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u'mightn', u'wasn', u'your', u'from', u'her', u'their', u'aren', u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u'but', u'don', u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u'these', u'up', u'will', u'below', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u'so', u'y', u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_arr=[]\n",
    "punc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in string.punctuation:\n",
    "    punc.append(k)\n",
    "# punc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in word_tokenize(str):\n",
    "    if (w not in stop_words) and (w not in punc):\n",
    "        filtered_arr.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tropical', 'Storm', 'Bill', 'hit', 'Gulf', 'Coast', 'United', 'States', 'summer', '2003', 'The', 'second', 'storm', 'Atlantic', 'hurricane', 'season', 'Bill', 'developed', 'tropical', 'wave', 'June', '29', 'north', 'Peninsula', 'It', 'slowly', 'organized', 'moved', 'northward', 'reached', 'peak', '60', 'mph', '95', 'km/h', 'shortly', 'making', 'landfall', 'south-central', 'Louisiana', 'It', 'produced', 'moderate', 'storm', 'surge', 'causing', 'tidal', 'flooding', 'In', 'Montegut', 'southeastern', 'portion', 'state', 'levee', 'breached', 'flooding', 'many', 'homes', 'Florida', 'two', 'swimmers', 'drowned']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PorterStemmer>\n"
     ]
    }
   ],
   "source": [
    "print(porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run running\n",
    "# play plaing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "print(porter.stem(\"running\").encode('ascii'))\n",
    "print(porter.stem(\"playing\").encode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tropical', 'Storm', 'Bill', 'hit', 'Gulf', 'Coast', 'United', 'States', 'summer', '2003', 'The', 'second', 'storm', 'Atlantic', 'hurricane', 'season', 'Bill', 'developed', 'tropical', 'wave', 'June', '29', 'north', 'Peninsula', 'It', 'slowly', 'organized', 'moved', 'northward', 'reached', 'peak', '60', 'mph', '95', 'km/h', 'shortly', 'making', 'landfall', 'south-central', 'Louisiana', 'produced', 'moderate', 'surge', 'causing', 'tidal', 'flooding', 'In', 'Montegut', 'southeastern', 'portion', 'state', 'levee', 'breached', 'flooding', 'many', 'homes', 'Florida', 'two', 'swimmers', 'drowned']\n"
     ]
    }
   ],
   "source": [
    "final_arr=[]\n",
    "for w in filtered_arr:\n",
    "    p=porter.stem(w).encode('ascii');\n",
    "    if p not in final_arr:\n",
    "        final_arr.append(w)\n",
    "print(final_arr)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample=state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize_sentence=nltk.word_tokenize('Mayank is really cool and he likes Apple,banana,oranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mayank', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('really', 'RB'),\n",
       " ('cool', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('he', 'PRP'),\n",
       " ('likes', 'VBZ'),\n",
       " ('Apple', 'NNP'),\n",
       " (',', ','),\n",
       " ('banana', 'NN'),\n",
       " (',', ','),\n",
       " ('oranges', 'NNS')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_tag=nltk.pos_tag(tokenize_sentence)\n",
    "pos_word_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunkParse=r'''Found(*CHUNK HERE*):{<NN.?>}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser=nltk.RegexpParser(chunkParse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunked=parser.parse(pos_word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Found(*CHUNK HERE*) Mayank/NNP)\n",
      "  is/VBZ\n",
      "  really/RB\n",
      "  cool/JJ\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  likes/VBZ\n",
      "  (Found(*CHUNK HERE*) Apple/NNP)\n",
      "  ,/,\n",
      "  (Found(*CHUNK HERE*) banana/NN)\n",
      "  ,/,\n",
      "  (Found(*CHUNK HERE*) oranges/NNS))\n"
     ]
    }
   ],
   "source": [
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_try=nltk.ne_chunk(pos_word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nn_try.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lematizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monkey\n",
      "good\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lematizer.lemmatize(\"monkeies\",'n'))\n",
    "print(lematizer.lemmatize(\"better\",'a'))\n",
    "print(lematizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sunonyms and antonyms from corpus wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre defined downloaded data with corpus\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "k=wordnet.synsets('program')\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n",
      "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(k[0].name())\n",
    "print(k[0].lemmas())\n",
    "print(k[0].lemmas()[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n",
      "[u'they drew up a six-step plan', u'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "# meanig of word\n",
    "print(k[0].definition())\n",
    "# example of word\n",
    "print(k[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to rpint synonyms and antonyms of a word\n",
    "syn=[]\n",
    "ant=[]\n",
    "def formatWord(word):\n",
    "    for w in wordnet.synsets(word):\n",
    "        for l in w.lemmas():\n",
    "            if l.name() not in syn:\n",
    "                syn.append(l.name())\n",
    "            if l.antonyms():\n",
    "                if l.antonyms()[0].name() not in ant:\n",
    "                    ant.append(l.antonyms()[0].name())\n",
    "    print(\"\\nSynonyms of: \"+word+\"\\n\")\n",
    "    print(syn)\n",
    "    print(\"\\nAntonyms of: \"+word+\"\\n\")\n",
    "    print(ant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synonyms of: good\n",
      "\n",
      "[u'good', u'goodness', u'commodity', u'trade_good', u'full', u'estimable', u'honorable', u'respectable', u'beneficial', u'just', u'upright', u'adept', u'expert', u'practiced', u'proficient', u'skillful', u'skilful', u'dear', u'near', u'dependable', u'safe', u'secure', u'right', u'ripe', u'well', u'effective', u'in_effect', u'in_force', u'serious', u'sound', u'salutary', u'honest', u'undecomposed', u'unspoiled', u'unspoilt', u'thoroughly', u'soundly']\n",
      "\n",
      "Antonyms of: good\n",
      "\n",
      "[u'evil', u'evilness', u'bad', u'badness', u'ill']\n"
     ]
    }
   ],
   "source": [
    "formatWord(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# similarity b/w two words\n",
    "def similarity(w1,w2):\n",
    "    k1=wordnet.synsets(w1)[0]\n",
    "    k2=wordnet.synsets(w2)[0]\n",
    "    print(k1.wup_similarity(k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909090909091\n",
      "0.695652173913\n",
      "0.125\n",
      "0.347826086957\n"
     ]
    }
   ],
   "source": [
    "similarity(\"ship\",\"boat\")\n",
    "similarity(\"ship\",\"car\")\n",
    "similarity(\"ship\",\"cycle\")\n",
    "similarity(\"ship\",\"bat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
